{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION METRICS \n",
    "\n",
    "## GPT CLASSIFIER EVALUATION METRICS\n",
    "\n",
    "### - ACCURACY  \n",
    "\n",
    "Calculate the accuracy of the intent classifier.    \n",
    "Accuracy measures the proportion of correctly classified cases from the total number of objects in the dataset.  \n",
    "Per prediction we compare 2 lists, the test labels, in e.g. ['flight'] or ['flight', 'airline'] with the model's  \n",
    "response, in e.g. ['flight', 'city', 'airline']. For each actual label present in the response we count +1 correct prediction else +1 incorrect predictions. \n",
    "The total number of prediction is calculated as matched_intent_count + mismatched_intent_count.   \n",
    "Thus, the amount of total predictions will be >= count of test records, since prediction count depends on the amount of actual labels per prompt.  \n",
    "\n",
    "\n",
    "### - PRECISION  \n",
    "\n",
    "Calculate precision for each class.  \n",
    "Precision is calculated as the fraction of instances   \n",
    "correctly classified as belonging to a specific class out of all instances   \n",
    "the model predicted to belong to that class (TP/(TP+FP)).  \n",
    "\n",
    "```  \n",
    "Example:  \n",
    "\n",
    "actual_intents = ['flight', 'airfare']    \n",
    "prediction = ['flight', 'airfare', 'flight_no']    \n",
    "\n",
    "Here 'flight_no' gets +1 FP and     \n",
    "classes `flight` & `airfare` earn +1 TP    \n",
    "```   \n",
    "\n",
    "### - RECALL\n",
    " \n",
    "Calculate recall for each class.    \n",
    "Recall is calculated as the fraction of instances in a class that the model correctly classified    \n",
    "out of all instances in that class (TP/(TP+FN)).  \n",
    "\n",
    "```\n",
    "Example:\n",
    "\n",
    "actual_intents = ['flight', 'airfare']  \n",
    "prediction = ['flight', 'city', 'flight_no']  \n",
    "\n",
    "Here 'airfare' gets +1 FN and   \n",
    "class `flight` earns +1 TP  \n",
    "```\n",
    "\n",
    "### - CONFUSION MATRIX  \n",
    "\n",
    "Calculate the confusion matrix.\n",
    "\n",
    "Given a prediction, all three prediction values, for example ['flight', 'flight_no', 'airport'], are counted for each test intent(s), such as [`flight_time`,`flight_no`].\n",
    "\n",
    "```\n",
    "Example:\n",
    "For test case `show all flights and fares from denver to san francisco` where:\n",
    "\n",
    "- actual_intents: ['flight', 'airfare']\n",
    "- predicted intents: ['flight', 'flight_no', 'airfare']\n",
    "\n",
    "We count all intents vs all predictions as `confusions` like this: \n",
    "\n",
    "- 'flight' -> 'flight'       +1\n",
    "- 'flight' -> 'flight_no'    +1\n",
    "- 'flight' -> 'airfare'     +1\n",
    "\n",
    "- 'airfare' -> 'flight'         +1\n",
    "- 'airfare' -> 'flight_no'      +1\n",
    "- 'airfare' -> 'airfare'       +1\n",
    "\n",
    "```\n",
    "\n",
    "Here, even if `airfare` & `flight` are correct predictions, we will still relate them with the (`flight`, `flight_no`) & (`airfare`, `flight_no`) respectively. \n",
    "\n",
    "\n",
    "By adopting this counting method, we are not only capturing the instances where classes align with the predictions accurately but also acknowledging   \n",
    "the correlations suggested by the model. This approach enables us to discern not only how frequently certain classes appear together    \n",
    "but also to identify any underlying associations the model implicitly recognizes.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero shot classifier \n",
    "\n",
    "Results:   \n",
    "\n",
    "`gpt-3.5-turbo`  \n",
    "`zero-shot`  \n",
    "`model_unknown_targets:  {'day_name'}`  \n",
    "\n",
    "**valid_res**: 845  -> times the model's response was in expected format  \n",
    "**invalid_res**:  3 -> times the model's response was malformed (excluded)  \n",
    "**general_accuracy**: 94%\n",
    "\n",
    "- [zero-shot-predictions](../model_evaluation/zero-shot_test_results.csv)  \n",
    "- [zero-shot-accuracy](../model_evaluation/zero-shot_accuracy.csv)  \n",
    "- [zero-shot-precision](../model_evaluation/zero-shot_precision.csv) (per class)  \n",
    "- [zero-shot-recall](../model_evaluation/zero-shot_recall.csv) (per class)  \n",
    "\n",
    "**evaluation cost for the whole test dataset**\n",
    "\n",
    " - based on $0.0005/1k input tokens and  $0.0015/1k output pricing for the `gpt-3-5-turbo`, \n",
    " - our input was 848 (requests) * 212 tokens each (on avg) == 179,7k input tokens \n",
    " - our expected output is fixed at 5-7 tokens depending on tokenizer  \n",
    "\n",
    "```\n",
    "For example for output: `[0, 3, 7]`  \n",
    " '[' and ']' would be tokens on their own.  \n",
    "'0', '2', and '7' would each be tokens on their own.  \n",
    "',' would also be a token on its own.  \n",
    "So, in total, the tokenizer would break the string '[0, 2, 7]' into 7 tokens.  \n",
    "```\n",
    "- our input was 848 (requests) * 7 each == 5,9k output tokens \n",
    "- 258,6 * 0.0015 = 0.09$ for input costs  \n",
    "- 5,9 * 0.0015 = 0.00885 for output costs   \n",
    "\n",
    "So, to evaluate the model with the `zero-shot` classifier on the complete test ds costs approximately `0,10$`  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few shot classifier \n",
    "\n",
    "Results:   \n",
    "\n",
    "`gpt-3.5-turbo`  \n",
    "`few-shot`  \n",
    "`model_unknown_targets:  {'day_name'}`  \n",
    "\n",
    "**valid_res**: 846  -> times the model's response was in expected format  \n",
    "**invalid_res**:  2 -> times the model's response was malformed (excluded)  \n",
    "**general_accuracy**: 95%\n",
    "\n",
    "- [few-shot-predictions](../model_evaluation/few-shot_test_results.csv)  \n",
    "- [few-shot-accuracy](../model_evaluation/few-shot_accuracy.csv)  \n",
    "- [few-shot-precision](../model_evaluation/few-shot_precision.csv) (per class)  \n",
    "- [few-shot-recall](../model_evaluation/few-shot_recall.csv) (per class)  \n",
    "\n",
    "\n",
    "**evaluation cost for the whole test dataset**\n",
    "\n",
    " - based on $0.0005/1k input tokens and  $0.0015/1k output pricing for the `gpt-3-5-turbo`, \n",
    " - our input was 848 (requests) * 305 tokens each (on avg) == 258,6k input tokens \n",
    " - our expected output is fixed at 5-7 tokens depending on tokenizer  \n",
    "\n",
    "```\n",
    "For example for output: `[0, 3, 7]`  \n",
    " '[' and ']' would be tokens on their own.  \n",
    "'0', '2', and '7' would each be tokens on their own.  \n",
    "',' would also be a token on its own.  \n",
    "So, in total, the tokenizer would break the string '[0, 2, 7]' into 7 tokens.  \n",
    "```\n",
    "- our input was 848 (requests) * 7 each == 5,9k output tokens \n",
    "- 258,6 * 0.0015 = 0.13$ for input costs  \n",
    "- 5,9 * 0.0015 = 0.00885 for output costs   \n",
    "\n",
    "So, to evaluate the model with the `few-shot` classifier on the complete test ds costs approximately `0,14$`  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
